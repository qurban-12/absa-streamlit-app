{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79IrZqHIWP8r",
        "outputId": "9cac6305-9515-49f0-ad8c-a0b4ea3ed2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=ae83c5d084f28b85678d403746b48f184945f841f01004166997207140cb5c03\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers librosa torch openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import whisper\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMhBZkZeXxVF",
        "outputId": "051265a3-7eff-4f5e-9187-f1ef9f9ae490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "ser_model_path = \"/content/drive/MyDrive/dlProject/Model/wav2vec2-ser-ravdess\"\n",
        "\n",
        "print(\"Loading SER Model (Wav2Vec2)...\")\n",
        "try:\n",
        "    ser_processor = Wav2Vec2Processor.from_pretrained(ser_model_path)\n",
        "    ser_model = Wav2Vec2ForSequenceClassification.from_pretrained(ser_model_path).to(device)\n",
        "    print(\"‚úÖ SER Model loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading SER model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epivuIG6X_HC",
        "outputId": "8d603930-ec73-41bb-8a3c-eecee00bb1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading SER Model (Wav2Vec2)...\n",
            "‚úÖ SER Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Load your trained RoBERTa (ABSA) Model\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "# PATH TO YOUR RO BERTA MODEL\n",
        "absa_model_path = \"/content/drive/MyDrive/dlProject/Model/RoBERTa_ABSA_Final\"\n",
        "\n",
        "print(\"Loading ABSA Model (RoBERTa)...\")\n",
        "try:\n",
        "    # ‚úÖ CORRECT: Use AutoModelForTokenClassification for extracting specific words\n",
        "    absa_tokenizer = AutoTokenizer.from_pretrained(absa_model_path)\n",
        "    absa_model = AutoModelForTokenClassification.from_pretrained(absa_model_path).to(device)\n",
        "    print(\"‚úÖ ABSA Model loaded successfully as Token Classifier!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading ABSA model: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU8szS8uYFz7",
        "outputId": "f5e4bd9c-f0d2-44b1-bd23-68cbfe265cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ABSA Model (RoBERTa)...\n",
            "‚úÖ ABSA Model loaded successfully as Token Classifier!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Loading Whisper ASR model...\")\n",
        "asr_model = whisper.load_model(\"base\")\n",
        "print(\"‚úÖ Whisper model loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ94Le7BYhP-",
        "outputId": "c08dfe35-da6a-4806-e44a-99ef777ec0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper ASR model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 139M/139M [00:00<00:00, 216MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Whisper model loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "absa_pipeline = pipeline(\n",
        "    \"token-classification\",\n",
        "    model=absa_model,\n",
        "    tokenizer=absa_tokenizer,\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "def get_audio_emotion(audio_path):\n",
        "    \"\"\"Predicts emotion from audio using Wav2Vec2\"\"\"\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = ser_processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = ser_model(**inputs).logits\n",
        "\n",
        "    pred_id = torch.argmax(logits, dim=-1).item()\n",
        "    return ser_model.config.id2label[pred_id]\n",
        "\n",
        "def get_text_aspect(text):\n",
        "    \"\"\"Extracts aspect only if it relates to FOOD/RESTAURANTS\"\"\"\n",
        "    results = absa_pipeline(text)\n",
        "\n",
        "    if not results:\n",
        "        return \"No Aspect Detected\"\n",
        "\n",
        "\n",
        "    detected_word = results[0]['word'].strip().lower()\n",
        "\n",
        "\n",
        "\n",
        "    category_map = {\n",
        "        # Food Quality keywords\n",
        "        \"food\": \"Food Quality\", \"soup\": \"Food Quality\", \"chicken\": \"Food Quality\",\n",
        "        \"taste\": \"Food Quality\", \"flavor\": \"Food Quality\", \"meal\": \"Food Quality\",\n",
        "        \"burger\": \"Food Quality\", \"pizza\": \"Food Quality\", \"meat\": \"Food Quality\",\n",
        "        \"sauce\": \"Food Quality\", \"dish\": \"Food Quality\", \"portion\": \"Food Quality\",\n",
        "\n",
        "        # Service keywords\n",
        "        \"waiter\": \"Service\", \"staff\": \"Service\", \"service\": \"Service\",\n",
        "        \"manager\": \"Service\", \"host\": \"Service\", \"chef\": \"Service\",\n",
        "\n",
        "        # Delivery/Time keywords\n",
        "        \"delivery\": \"Delivery\", \"time\": \"Delivery/Time\", \"late\": \"Delivery/Time\",\n",
        "        \"slow\": \"Delivery/Time\", \"fast\": \"Delivery/Time\", \"minute\": \"Delivery/Time\",\n",
        "\n",
        "        # Price keywords\n",
        "        \"price\": \"Price\", \"cost\": \"Price\", \"money\": \"Price\",\n",
        "        \"bill\": \"Price\", \"expensive\": \"Price\", \"cheap\": \"Price\",\n",
        "\n",
        "        # Ambience keywords\n",
        "        \"ambience\": \"Ambience\", \"place\": \"Ambience\", \"music\": \"Ambience\",\n",
        "        \"atmosphere\": \"Ambience\", \"clean\": \"Ambience\", \"table\": \"Ambience\"\n",
        "    }\n",
        "\n",
        "\n",
        "    if detected_word in category_map:\n",
        "        return category_map[detected_word]\n",
        "    else:\n",
        "        return \"Irrelevant / Non-Food Domain\"\n",
        "\n",
        "def run_full_pipeline(audio_path):\n",
        "    print(f\"\\n--- Processing: {audio_path.split('/')[-1]} ---\")\n",
        "\n",
        "\n",
        "    emotion = get_audio_emotion(audio_path)\n",
        "\n",
        "\n",
        "    print(\"üìù Transcribing audio...\")\n",
        "    transcription = asr_model.transcribe(audio_path)[\"text\"]\n",
        "    print(f\"   Transcript: \\\"{transcription.strip()}\\\"\")\n",
        "\n",
        "    aspect_sentiment = get_text_aspect(transcription)\n",
        "\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    if aspect_sentiment == \"Irrelevant / Non-Food Domain\":\n",
        "        print(f\"‚ö†Ô∏è  NOTE: Input does not appear to be about food.\")\n",
        "        print(f\"üöÄ RAW OUTPUT: User is {emotion}, but no food aspect was found.\")\n",
        "    else:\n",
        "        print(f\"üß† Detected Aspect: {aspect_sentiment}\")\n",
        "        print(f\"üöÄ FINAL OUTPUT: Customer is {emotion} about {aspect_sentiment}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5g9eq1jgpfr",
        "outputId": "b2a782db-a294-4f03-8d58-a643656e5f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "id2label = {0: \"O\", 1: \"B-ASP\", 2: \"I-ASP\"}\n",
        "\n",
        "def extract_aspects_custom(text, model, tokenizer):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    extracted_aspects = []\n",
        "    current_aspect = []\n",
        "\n",
        "    for token, label_id in zip(tokens, predictions):\n",
        "        label = id2label[label_id]\n",
        "\n",
        "        clean_token = token.replace(\"ƒ†\", \" \").replace(\"ƒä\", \"\").strip()\n",
        "\n",
        "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            continue\n",
        "\n",
        "        if label == \"B-ASP\":\n",
        "            if current_aspect:\n",
        "                extracted_aspects.append(\" \".join(current_aspect).strip())\n",
        "            current_aspect = [clean_token]\n",
        "        elif label == \"I-ASP\":\n",
        "            current_aspect.append(clean_token)\n",
        "        else:\n",
        "            if current_aspect:\n",
        "                extracted_aspects.append(\" \".join(current_aspect).strip())\n",
        "                current_aspect = []\n",
        "\n",
        "    if current_aspect:\n",
        "        extracted_aspects.append(\" \".join(current_aspect).strip())\n",
        "\n",
        "    return extracted_aspects\n",
        "\n",
        "def get_audio_emotion(audio_path):\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = ser_processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = ser_model(**inputs).logits\n",
        "\n",
        "    pred_id = torch.argmax(logits, dim=-1).item()\n",
        "    return ser_model.config.id2label[pred_id]\n",
        "\n",
        "def run_full_pipeline(audio_path):\n",
        "    print(f\"\\n--- Processing: {audio_path.split('/')[-1]} ---\")\n",
        "\n",
        "    emotion = get_audio_emotion(audio_path)\n",
        "    print(f\"üîä Detected Vocal Emotion: {emotion}\")\n",
        "\n",
        "    print(\"üìù Transcribing audio...\")\n",
        "    transcription = asr_model.transcribe(audio_path)[\"text\"].strip()\n",
        "    print(f\"   Transcript: \\\"{transcription}\\\"\")\n",
        "\n",
        "    aspects_found = extract_aspects_custom(transcription, absa_model, absa_tokenizer)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    if not aspects_found:\n",
        "        print(f\"üöÄ FINAL OUTPUT: Customer is {emotion}, but no specific food aspect was found.\")\n",
        "    else:\n",
        "        aspect_string = \", \".join(aspects_found)\n",
        "        print(f\"üß† Detected Aspects: {aspects_found}\")\n",
        "        print(f\"üöÄ FINAL OUTPUT: Customer is {emotion} about: {aspect_string}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "pokAzz11iYUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# REPLACE with your actual test audio file path\n",
        "test_file = \"/content/drive/MyDrive/dlProject/Datasets/moiz.wav\"\n",
        "\n",
        "# Check if file exists before running\n",
        "import os\n",
        "if os.path.exists(test_file):\n",
        "    run_full_pipeline(test_file)\n",
        "else:\n",
        "    print(f\"File not found: {test_file}\")\n",
        "    print(\"Please upload a .wav file to your Drive and update the 'test_file' path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C3AwUIJkh5F",
        "outputId": "5cb407bf-2048-485f-b498-1ac06b67ea36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing: moiz.wav ---\n",
            "üîä Detected Vocal Emotion: angry\n",
            "üìù Transcribing audio...\n",
            "   Transcript: \"Your food is disgusting!\"\n",
            "------------------------------\n",
            "üß† Detected Aspects: ['food']\n",
            "üöÄ FINAL OUTPUT: Customer is angry about: food\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define Labels\n",
        "id2label = {0: \"O\", 1: \"B-ASP\", 2: \"I-ASP\"}\n",
        "\n",
        "# 2. DEFINE YOUR ALLOWLIST (The \"Food Domain\" Filter)\n",
        "# The pipeline will ONLY show aspects if they (or their singular form) are in this list.\n",
        "VALID_RESTAURANT_TERMS = {\n",
        "    # Food & Drink\n",
        "    \"food\", \"dish\", \"meal\", \"pizza\", \"pasta\", \"burger\", \"chicken\", \"meat\",\n",
        "    \"steak\", \"soup\", \"salad\", \"sauce\", \"crust\", \"dessert\", \"cake\", \"ice cream\",\n",
        "    \"drink\", \"wine\", \"beer\", \"coffee\", \"tea\", \"menu\", \"portion\", \"flavor\", \"taste\",\n",
        "\n",
        "    # Service & Staff\n",
        "    \"service\", \"staff\", \"waiter\", \"waitress\", \"server\", \"host\", \"manager\", \"chef\",\n",
        "    \"delivery\", \"management\",\n",
        "\n",
        "    # Ambience & Place\n",
        "    \"ambience\", \"atmosphere\", \"place\", \"restaurant\", \"setting\", \"music\", \"lighting\",\n",
        "    \"table\", \"seat\", \"seating\", \"view\", \"noise\", \"decor\",\n",
        "\n",
        "    # Money\n",
        "    \"price\", \"cost\", \"value\", \"bill\", \"check\", \"money\"\n",
        "}\n",
        "\n",
        "def extract_aspects_custom(text, model, tokenizer):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    extracted_aspects = []\n",
        "    current_aspect = []\n",
        "\n",
        "    for token, label_id in zip(tokens, predictions):\n",
        "        label = id2label[label_id]\n",
        "\n",
        "        # Clean token\n",
        "        clean_token = token.replace(\"ƒ†\", \" \").replace(\"ƒä\", \"\").strip()\n",
        "\n",
        "        # Skip special tokens\n",
        "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            continue\n",
        "\n",
        "        if label == \"B-ASP\":\n",
        "            if current_aspect:\n",
        "                full_word = \" \".join(current_aspect).strip()\n",
        "                extracted_aspects.append(full_word)\n",
        "            current_aspect = [clean_token]\n",
        "        elif label == \"I-ASP\":\n",
        "            current_aspect.append(clean_token)\n",
        "        else:\n",
        "            if current_aspect:\n",
        "                full_word = \" \".join(current_aspect).strip()\n",
        "                extracted_aspects.append(full_word)\n",
        "                current_aspect = []\n",
        "\n",
        "    if current_aspect:\n",
        "        full_word = \" \".join(current_aspect).strip()\n",
        "        extracted_aspects.append(full_word)\n",
        "\n",
        "    # --- FILTERING STEP ---\n",
        "    # Only keep aspects that are in our VALID list\n",
        "    final_aspects = []\n",
        "    for aspect in extracted_aspects:\n",
        "        # Check if the lowercase aspect (or singular version) is in our list\n",
        "        # This allows \"pizzas\" to pass if \"pizza\" is in the list\n",
        "        norm_aspect = aspect.lower()\n",
        "        if norm_aspect in VALID_RESTAURANT_TERMS or norm_aspect.rstrip('s') in VALID_RESTAURANT_TERMS:\n",
        "            final_aspects.append(aspect)\n",
        "\n",
        "    return final_aspects\n",
        "\n",
        "def get_audio_emotion(audio_path):\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "    inputs = ser_processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = ser_model(**inputs).logits\n",
        "\n",
        "    pred_id = torch.argmax(logits, dim=-1).item()\n",
        "    return ser_model.config.id2label[pred_id]\n",
        "\n",
        "def run_full_pipeline(audio_path):\n",
        "    print(f\"\\n--- Processing: {audio_path.split('/')[-1]} ---\")\n",
        "\n",
        "    emotion = get_audio_emotion(audio_path)\n",
        "    print(f\"üîä Detected Vocal Emotion: {emotion}\")\n",
        "\n",
        "    print(\"üìù Transcribing audio...\")\n",
        "    transcription = asr_model.transcribe(audio_path)[\"text\"].strip()\n",
        "    print(f\"   Transcript: \\\"{transcription}\\\"\")\n",
        "\n",
        "    # Extract AND Filter\n",
        "    aspects_found = extract_aspects_custom(transcription, absa_model, absa_tokenizer)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    if not aspects_found:\n",
        "        print(f\"üöÄ FINAL OUTPUT: Customer is {emotion}, but no relevant food aspect was found.\")\n",
        "    else:\n",
        "        aspect_string = \", \".join(aspects_found)\n",
        "        print(f\"üß† Detected Aspects: {aspects_found}\")\n",
        "        print(f\"üöÄ FINAL OUTPUT: Customer is {emotion} about: {aspect_string}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "bHVz4m5ekNaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# REPLACE with your actual test audio file path\n",
        "test_file = \"/content/drive/MyDrive/dlProject/Datasets/03-01-06-02-01-02-14.wav\"\n",
        "\n",
        "# Check if file exists before running\n",
        "import os\n",
        "if os.path.exists(test_file):\n",
        "    run_full_pipeline(test_file)\n",
        "else:\n",
        "    print(f\"File not found: {test_file}\")\n",
        "    print(\"Please upload a .wav file to your Drive and update the 'test_file' path.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_KmSbJyY5aL",
        "outputId": "3b6da629-bc19-4ead-c25b-fe9196604edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Processing: 03-01-06-02-01-02-14.wav ---\n",
            "üîä Detected Vocal Emotion: fearful\n",
            "üìù Transcribing audio...\n",
            "   Transcript: \"Kids are talking by the door!\"\n",
            "------------------------------\n",
            "üöÄ FINAL OUTPUT: Customer is fearful, but no relevant food aspect was found.\n",
            "------------------------------\n"
          ]
        }
      ]
    }
  ]
}